{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"google/gemma-3-1b-it\"\n",
    "model_name = \"gemma_1b_reasonning\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Since I don't have enough ressources\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                        bnb_4bit_use_double_quant=True,\n",
    "                                        bnb_4bit_quant_type=\"nf4\",\n",
    "                                        bnb_4bit_compute_dtype=torch.bfloat16) \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"  # ‚Üê add this\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=50, temperature=0.7):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What's the capital of France?\\n\\n**Paris**\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What's the capital of France?\"\n",
    "generate_response(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def prune_layers(model, start_layer, end_layer):\n",
    "    \n",
    "    # Keep only layers outside the pruning range\n",
    "    pruned_layers = nn.ModuleList(\n",
    "        [layer for idx, layer in enumerate(model.model.layers) \n",
    "         if idx < start_layer or idx >= end_layer]\n",
    "    )\n",
    "    print(len(pruned_layers))\n",
    "    # Assign back to the model\n",
    "    model.model.layers = pruned_layers\n",
    "    model.config.num_hidden_layers = len(pruned_layers)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "pruned_model = prune_layers(model, 9, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"what's the capital of france?\\n\\nThe capital's France**\\n\\nThe following are the common years of the capital's France?\\n\\n0100128400128400000000000000\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"what's the capital of france?\"\n",
    "generate_response(pruned_model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3DecoderLayer(\n",
       "  (self_attn): Gemma3Attention(\n",
       "    (q_proj): Linear4bit(in_features=1152, out_features=1024, bias=False)\n",
       "    (k_proj): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
       "    (v_proj): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
       "    (o_proj): Linear4bit(in_features=1024, out_features=1152, bias=False)\n",
       "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "  )\n",
       "  (mlp): Gemma3MLP(\n",
       "    (gate_proj): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
       "    (up_proj): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
       "    (down_proj): Linear4bit(in_features=6912, out_features=1152, bias=False)\n",
       "    (act_fn): PytorchGELUTanh()\n",
       "  )\n",
       "  (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "  (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "  (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "  (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model.model.layers[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                          # LoRA rank, lower is lighter-weight\n",
    "    lora_alpha=16,                # alpha scaling factor (usually 2*r)\n",
    "    lora_dropout=0.1,             # dropout regularization\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # attention layers\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"       # MLP layers\n",
    "    ],\n",
    "    bias=\"none\",                  # LoRA typically ignores biases\n",
    "    layers_to_transform=list(range(len(pruned_model.model.layers))),\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = get_peft_model(pruned_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"what's the capital of france?\\n\\nThe date of issuance of a royal/official proclamation is a common occurrence.\\n\\n**French**\\n\\n**13th, 13th, 14th, 14th, 17th, 14th\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"what's the capital of france?\"\n",
    "generate_response(pruned_model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "local_dataset = load_from_disk(\"./c4_10k_subset\")\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = local_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(pruned_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100it [01:10,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 99/125 completed. Average Loss: 4.2245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 125it [01:27,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average Loss: 4.0607\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fine-tuning loop\n",
    "epochs = 1  # 1 epoch is often enough for healing\n",
    "\n",
    "pruned_model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for idx, batch in tqdm(enumerate(dataloader), desc=f\"Epoch {epoch+1}\"):\n",
    "        \n",
    "        batch = {k: v.to(pruned_model.device) for k, v in batch.items()}\n",
    "        outputs = pruned_model(**batch, labels=batch[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            print(\"Loss is NaN at batch\", idx, batch)\n",
    "            continue\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss.item()\n",
    "     \n",
    "        \n",
    "        if (idx+1)%100==0 :\n",
    "            print(f\"batch {idx}/{len(dataloader)} completed. Average Loss: {epoch_loss / idx:.4f}\")\n",
    "            \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save fine-tuned model adapters\n",
    "pruned_model.save_pretrained(\"gemma_pruned_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France? 1500 1200 1800 1994 1995 1996 1998 1999 1999 2000 '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model.eval()\n",
    "prompt = \"What is the capital of France? \"\n",
    "\n",
    "generate_response(pruned_model, tokenizer, prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio2audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
